# -*- coding: utf-8 -*-
"""Working_Llama-3.2-1B.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mitAStAyQD69ZWsY5HhFDlG40rlABkni
"""

!pip install -U transformers

"""## Local Inference on GPU
Model page: https://huggingface.co/meta-llama/Llama-3.2-1B

âš ï¸ If the generated code snippets do not work, please open an issue on either the [model repo](https://huggingface.co/meta-llama/Llama-3.2-1B)
			and/or on [huggingface.js](https://github.com/huggingface/huggingface.js/blob/main/packages/tasks/src/model-libraries-snippets.ts) ðŸ™

The model you are trying to use is gated. Please make sure you have access to it by visiting the model page.To run inference, either set HF_TOKEN in your environment variables/ Secrets or run the following cell to login. ðŸ¤—
"""

from huggingface_hub import login
login(new_session=False)

# Use a pipeline as a high-level helper
from transformers import pipeline
import os
from google.colab import userdata

# Set the HF_TOKEN environment variable
os.environ["HF_TOKEN"] = userdata.get("HF_TOKEN")

pipe = pipeline("text-generation", model="meta-llama/Llama-3.2-1B")

# Load model directly
from transformers import AutoTokenizer, AutoModelForCausalLM

tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-3.2-1B")
model = AutoModelForCausalLM.from_pretrained("meta-llama/Llama-3.2-1B")

# load files temporarily
from google.colab import files
uploaded = files.upload()

# unzip
!unzip /content/pdfs_for_chatbot.zip

# Install dependencies
!pip install langchain langchain-community pypdf chromadb sentence-transformers transformers

from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter

import os
os.listdir("/content")

# Load all pdfs

pdf_folder = "/content/pdfs_for_chatbot"

docs = []
for file in os.listdir(pdf_folder):
    if file.endswith(".pdf"):
        loader = PyPDFLoader(os.path.join(pdf_folder, file))
        docs.extend(loader.load())

print(f"Loaded {len(docs)} PDF pages")

# Split long text into smaller chunks
splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
split_docs = splitter.split_documents(docs)

print(f"Created {len(split_docs)} text chunks")

# wrap
from langchain_community.llms import HuggingFacePipeline
llm = HuggingFacePipeline(pipeline=pipe)

# embedding model
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma

embedding_model = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")

# Create a persistent Chroma DB
vectordb = Chroma.from_documents(split_docs, embedding=embedding_model, persist_directory="chroma_db")

vectordb.persist()

# If chunks print then working

results = vectordb.similarity_search("What is this document about?", k=2)
for i, r in enumerate(results, 1):
    print(f"\nResult {i}:\n{r.page_content[:500]}...")

# make retriever --> return 3 chunks? change this?
retriever = vectordb.as_retriever(search_kwargs={"k": 3})

from langchain.chains import RetrievalQA

# connect llama with chroma
qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=retriever,
    chain_type="stuff"
)

# test it!
result = qa_chain.invoke({"query": "Summarize the key findings from the PDFs"})
print(result["result"])